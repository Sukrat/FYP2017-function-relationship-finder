Many designers are increasingly utilizing dynamic hardware adaptations to improve performance while limiting the power consumption. Whereas Some are using software to decrease power usage for e.g. putting the system in sleep mode when it's in idle state. The main goal remains the same, which is to extract maximum performance while minimizing the temperature and power. The study and examination of the events affecting the consumption will help us to predict and minimize the consumption of energy with very high accuracy.

\section{Energy consumption and Performance Events}

First let's look at energy consumption. Energy consumption is the power (usually in watts) consumed by a system. This system could be the processor/CPU, memory, disk, I/O (Input/Output) system, chipset or the whole computer system itself. It has been known that power consumption of any system has a high correlation to its usage. Since the correlation is high for its usage. It means usage is a is a good way to understand the power consumption. Performance events are one of the better ways to measure the usage of a system as they are known much more faster than the temperature sensors. As temperature sensors are slow in response due to the thermal inertial of the micro processor.

Now let's look at what are performance events, performance events are any events that affect the consumption of energy in some way. Selection of performance events is quite challenging. A simple example would be the effect of cache misses in the processor. For a typical processor, the highest level of cache would be L3 or L2 depending on the type of processor. Now for some transaction which could not be found in the highest level of cache (cache miss) would cause a cache block size access to the main memory. Thus, number of main memory access would be directly proportional to the cache misses. Since these memory access is off-chip, power is consumed in the memory controller and DRAM. Even though, the relation is not simple as it seems but a strong casual linear relationship between the cache miss and the main memory power consumption~\cite{bircher2007complete}.

Another performance event can be instructions executed. The more instruction being executed, will turn on and use more units of the system. Hence, power is consumed as opposed to when the processor is in its idle state~\cite{gilberto2005power}.

Cache miss, TLB misses are also a good performance events as they seem to have a strong relationship between the power consumption as processor needs to handle memory page walks. Same can be said for Page faults where a program is not able to find mapped address in physical memory as it has not been loaded yet. This causes a trap which can result into number of situations, one of them which is to get the data from disk. In simple terms it is longer walk from cache miss. This walk to the disk and raising of exceptions would consume more energy by the disk as well as the CPU. Another thing to note here most of the relations that we saw above are directly proportional to each other. Increase in the variable like cache miss, number of cycles in CPU etc gives rise to energy consumption by the system. This makes linear model a good point to start when analysing the relationship between the performance events variables and energy consumption.

\section{Related Work}

In this section, some of the prior researches are discussed. Hardware performance counter's links to processor power consumption was first demonstrated by Bellosa. In \cite{bellosa2000benefits}, Bellosa demonstrated the high correlation between performance counters such as memory references, L2 cache, floating point operations to processor power consumption.

Gilbert in \cite{gilberto2005power}, predicted the power consumption for Intel XScale processors using performance monitoring unit events. Since power consumption is greatly dependent on executing workload, power estimation was done using HPCs (Hardware performance counters) such as Instruction cache miss, TLB misses etc. Linear parmaterisation of power consumption based on performance events was done based on performance events. 

In \cite{yang2016performance}, proposed a full system power model for CPU-intensive and memory intensive applications with active cycles, instruction retired and LLC missed as performance events. A full power model for I/O intensive applications was also proposed considering the system level utilization as a complement for performance events. Many machine learning based algorithms like logistic regression, elastic net and k-nearest neighbours were applied to real world application. 

Bircher approached in a distinct way by using events local to the processor and eliminating the need for sensors spread across various parts of the systems \cite{bircher2012complete}. Linear regression modeling was done in order to predict the power consumption at runtime. Multiple linear and polynomial regression was done only when accuracy was not obtained. 

High correlation between performance events and power consumption was demonstrated by all of them. But all of them tried to predict the power consumption using sytem events. They use various methodolgies in predicting so. In \cite{yang2016performance}, a number of machine learning algorithms were used. But in this project, an attempt is made to understand the monotonic relation between the events and power. The predictive models would work on a particular specification of system. But understanding the relation will help define a model on the architecture level. This will also help in verifying the models that already exist.


\section{Existence of functional relation}

\textbf{Definition}: \textit{Given a dataset of pairs \((x_i, y_i)\) where \(i \in [1, n]\) of two variables \(x\) and \(y\), and the range \(X\) of \(x\), \(y\) is a function of \(x\) iff for each \(x_0 \in X\), there is exactly one value of \(y\), say \(y_0\), such that \((x_0, y_0)\) is in dataset.}~\cite{zembowicz1993testing}

Above is the definition of functional relation. In our case the \(x_i = (p_1, p_2, \ldots p_k)\) and \(y_i = E_i\) where \(p\) are the performance events and \(k\) is the number of events and \(E\) is the energy consumption.

In other words, functional relationship is an one to one mapping between our input variables \(p_1, p_2, \ldots p_k\) and output \(E_i\). This reasoning can be explained quite intuitively as assuming functional relation we can formulate a \(f(p_1, p_2, \ldots p_k) = E_i\) where \(f\) is function. Now if one \((p_1, p_2, \ldots p_k)\) can give more than one output \(E_i\) then that \(f\) function is either not correct or \(f\) does not exist. A question that immediately arises is what if two different \((p_1, p_2, \ldots p_k)\) gives same output \(E_i\). The answer is it is possible and it doesnot violate the functional relation definition as there is still 1 to 1 mapping from input to output. The only difference is the functional relation is not surjective any more which means that one cannot figure out the input values from output values (i.e. the other way around). But we are only interested in predicting the output rather than inputs from the output variables.

The Proof of the existence of functional relation is given below:

But first let's look at our dataset that will be provided.
We know that the data will be in the following format:

Let \(k\) be the number of parameters for the energy and \(n\) be the number of records in the dataset

\(E_1,\ x_{11},\ x_{12},\ \ldots x_{1k}\)\\
\(E_2,\ x_{21},\ x_{22},\ \ldots x_{2k}\)\\
\ldots\\
\(E_n,\ x_{n1},\ x_{n2},\ \ldots x_{nk}\)\\
where \(E_n\) is the dynamic energy for the nth tuple and \(x_{nk}\) corresponds to the \(k\)th performance event for \(n\)th record.

We will use mathematical definition of functional relationship given above to prove the approaches:

\textbf{To Prove:} We need to prove that finding atleast 2 equal performance events with different dynamic energies ensures that there exists no functional relationship in the dataset.

\begin{proof}
	Let us assume that there exists a functional relation such that:\\
	\(f(x_{n1},\ x_{n2},\ \ldots x_{nk}) = E_n\)\\
	where \(f\) is the functional relation for the dataset.

	If we have \(f(x_{i1},\ x_{i2},\ \ldots x_{ik}) = E_i\) and \(f(x_{j1},\ x_{j2},\ \ldots x_{jk}) = E_j\)
	where \(E_i \neq E_j\) and \((x_{i1},\ x_{i2},\ \ldots x_{ik}) = (x_{j1},\ x_{j2},\ \ldots x_{jk})\)

	If such \(i\) and \(j\) exists. Then, we can conclude that the \(f\) is not a function by using the definition of a function as this assumed function has two images.

	Which contradicts from the hypothesis stated above.
	Hence by proof of contradiction we could say that \(f\) is not a function on the dataset.
\end{proof}

Now the task is to find similar \(x_{11},\ x_{12},\ \ldots x_{1k}\) input variables. If it was equality, it could be performed trivially by sorting the data records on the basis of \(x_{11},\ x_{12},\ \ldots x_{1k}\). Followed by going throught the data records in that order to find equal records. As equal records will be next to each other when sorted.

But the dataset accumulated is collected from experimental setup. Experimental setups data have some error/tolerance associated to it. As the equipment or software cannot accurately measure and have some error associated to it. e.g. Energy consumption is measured by a power meter which does have a confidence interval. The tolerance associated with the dataset no longer allows to perform equality on input variables. Hence, a method is needed to measure the equality with the tolerance.

Now after knowing the tolerances the data records in the dataset now looks more of the form:

\(E_n \pm e_E,\ x_{n1} \pm e_1,\ x_{n2} \pm e_2,\ \ldots x_{nk} \pm e_n\)\\
where \(e\) is now the error associated with the variable.

Hence dataset must be clustered with the tolerances associated to them. Data points having similar \(k\) parameters will fall in the same cluster. The output variable \(E_n \pm e_E\) must be close to each other within some tolerance for all the points in the cluster. If they are not, it violates the functional relation and the data points are picked and must be given to the user to either discard if corrupt. This removal of corrupt data records is known as Data cleanups. If they are not corrupt then it proves the non-functional relation in the dataset. Analysis of functional relation can be done when no records are found that contradicts the functional relation definition.

\section{Analysing functional relation}

Now this section corresponds to analysing the functional relation. A functional relation can of many forms, there can be logarithmic functional relation, exponential, linear, polynomial etc. According to \cite{bircher2007complete}, shows that functional relation between performance event and dynamic energy is of the linear form. This is due to the trickle down effect of the performance events. Hence, Linear model is used to understand the relationship between the events and the consumption. 

This project is more interested in finding whether a strong relation of monotonicity exists in the dataset or not. And linear regression are best suited for this kind of task. Linear regression is an approach determine how strongly two variables correlate with each other. Correlation not necessarily means causation. Looking at the data points and linear regression values such as Pearson coefficient, $R^2$ can determine how strong the correlation is between 2 values but it cannot explain the cause of it.

Dataset contains \(k\) parameter variables, to analyse relation between the parameter variables and the output variable. One parameter is chosen at a time. To see the change in the parameter variable chosen and output variable, isolation of the other variables is required. Isolation is done by grouping the data by \(k-1\) parameter variable forming a number of clusters with similar \(k-1\) paramters. These clusters are then visited and linear regression is performed on the \(k^{th}\) variable and the output variable. 

From the steps above, it can be observed the parameter to be analysed is isolated by grouping the dataset with the variables that are not being analysed and are equivalent. Since, clustering of dataset is performed again but with different vector dimension of \(k-1\), the clustering algorithms are needed by both the objectives.

\section{Clustering Methodologies}

This section introduces two clustering algorithm that was used to cluster data points which are similar or close to each other needed by both of the above objectives. There are many clustering methodologies out there \cite{xu2005survey}. No clustering algorithm can be universally solve all the problems. Algorithm which favor certain observations, assumptions and favor some type of biases are designed and used. 

Clustering involves grouping similar data by their attributes. There were number of clustering algorithms like Heirarchial clustering, K means clustering, Graph based clustering e.g. Chameleon. But we chose, Grid based clustering and Distance based clustering.

Most of clustering algorithms like K means clustering requires the number of clusters to be formed. In the dataset, number of clusters need to be form is not known. In Heirarchial clustering, heirarchy of dataset is used but the relation is functional and not heirarchial. Graph based clustering usually requires edges, and edges could be formed but they would increase the space complexity exponential. As more storage will be required to store the edges of the dataset.

The two types of clustering algorithms chosen, uses some facts about the dataset. Datasets with tolerances for clustering would be best suited to use these algorithms.

\subsection{Grid based clustering}

Since tolerance must be used as a measure of clustering datapoints. The definition of equality for a variable changes from \(x_{ia} = x_{ja}\) to \(|x_{ia} - x_{ja}| \leq e_a\). Now, if simple sort and search for similar variable would be employed, it would not work correctly as equivalent records would not be next to each other. They might non-equivalent records in between.

Example data to illustrate the same:
\begin{center}
	\begin{tabular}{ | c | c | c |}
		\hline
		\(E\) & \(x_1\) & \(x_2\) \\ \hline
		3.5   & 4.5     & 6.5     \\\hline
		4.1   & 4.6     & 10.6    \\\hline
		0.2   & 4.7     & 6.5     \\\hline
		1.6   & 4.7     & 7.6     \\\hline
		\hline
	\end{tabular}
\end{center}

The above three records are sorted by their parameters \(x_1, x_2\). We can see that if the \(e = 0.5\) is the absolute error. Then record number 1 and 3 are similar to each other but donot lie next to each other. This increases the complexity of finding similar records from \(O(N \log(N))\) to \(O(N^2)\) as we donot know where the similar records will lie, so N into N search must be performed which is quite inefficient.

To make it effecient, instead of finding similar records in the dataset. The whole \(k-dimensional\) space is divided into small \(k-cubes\) whose dimensions are \((e_1 * e_2 * \ldots e_k)\). Each \(k-cube\) has its own integer coordinates in space. The data records are grouped together with respect to their respective \(k-cube\) coordinates. Since the coordinates are integers (equality can be performed). They can be grouped in \(O(N\log(N))\) time. The calculation of the coordinates in which the data point belongs to can be calculated in \(O(1)\) time by the following.

\[coordinate = (\Bigl\lfloor \frac{x_{n1}}{e_1} \Bigl\rfloor, \Bigl\lfloor \frac{x_{n2}}{e_2} \Bigl\rfloor, \ldots \Bigl\lfloor \frac{x_{nk}}{e_k} \Bigl\rfloor)\]

Every data point will have a corresponding coordinate they belong to. Data points are then grouped together by their coordinates. Each and every grouped coordinate is a \(k-dimensional\) cube. 

\begin{algorithm}
	\caption{Grid based clustering}\label{alg:gridExistence}
	\begin{algorithmic}[1]
		\Procedure{grid}{$a,b$}
		\State \textbf{Input} $dataset$: a list of data points, $e$: errors for each variable
		\State \textbf{Result} clusters created with each cluster having its unique index
		\State $list \gets List.empty$
		\For{\textbf{each} $point$ \textbf{in} $dataset$} \Comment{$N$ iterations}
		\State $coordinate \gets (\lfloor point.x_{1}/e_1 \rfloor, \ldots \lfloor point.x_{k}/e_k \rfloor)$
		\State $list.add([coordinate, point])$
		\EndFor
		\State sort $list$ by the  $coordinate$ value \Comment{$N \log(N)$ for sorting}
		\State $result \gets$ group $list$ by the  $coordinate$ value \Comment{$N$ iterations}
        \State\Return $result$
		\EndProcedure
	\end{algorithmic}
\end{algorithm}

It can be seen, since the space is divided into cube of certain dimension. The cubes are disjoint and they donot overlap. It can be imagined as a building of boxes stack on top and side of each other. There will be points which lie on the edges of the \(n-cube\). Since the cubes donot overlap. They will be close to some of points in the cubes next to them. But in the algorithm one data point is assigned to only one cluster. This problem is solved by the next approach which is the distance based clustering. This problem comes with performance advantage. 

The algorithm complexity can be measured. 
\begin{itemize}
	\item Time complexity \(O(N log(N))\): This is because of sorting which is the slowest operation in the algorithm, but neverthless optimizations can be done by using a Dictionary or Binary search tree to group data with equal coordinates. Using Dictionary will give complexity of \(O(N)\).
	\item Space complexity \(O(N)\): For each data point only the corresponding \(coordinate\) is stored.
\end{itemize}

\subsection{Distance based clustering}

In this method, data points are thought of as vectors in euclidean space of \(n-dimension\) where \(n\) is number of parameters considered when clustering. The equality of vectors according to which clustering should take place is done by using the euclidean distance between two vectors. Two vectors are said to be equivalent if the distance between the two vectors is less than the tolerance specified.

Let \(u\) and \(v\) be two vectors in space with dimension \(n\),\\
then the euclidean distance is define as: \\
\[dist(u, v) = \sqrt{(u[1] - v[1])^2 + (u[2] - v[2])^2 + \ldots (u[n] - v[n])^2}\]

Now, two vectors \(u\) and \(v\) are said to be equal or close enough when:\\
\[dist(u,v) \leq tol\]  where \(tol\) is the maximum distance between two points to call them neighbours.

This tolerance can be thought of as the total tolerance of the between two data points. Because, the difference between two points in all of their different dimensions are squared and ``added'' together and then square rooted. It signifies the total tolerance that is allowed between two datapoints. 

This clustering is creating spheres of \(k-dimension\) for each data point. Here, overlapping of spheres is allowed. Incuring huge performance cost.

\begin{algorithm}
	\caption{Distance based clustering}\label{alg:dbscanExistence}
	\begin{algorithmic}[1]
		\Procedure{distance\_based}{$a,b$}
		\State \textbf{Input} $dataset$: a list of data points, $tol$: tolerance, $oTol$: output tolerance
		\State \textbf{Result} subset of the data points that violate functional relation
		\State $cluster \gets Dictionary.empty$
		\For{\textbf{each} $point$ \textbf{in} $dataset$} \Comment{$N$ iterations}
		\State $cluster[point] \gets List.empty$ 	
        \For{\textbf{each} $neighbour$ \textbf{in} $dataset$} \Comment{$N$ iterations}
        \If{$dist(point, neighbour) < tol$}
        \State $cluster[point].add(neighbour)$
        \EndIf
        \EndFor
        \EndFor
        \State\Return $cluster$
		\EndProcedure
	\end{algorithmic}
\end{algorithm}


The algorithm complexity can be measured. 
\begin{itemize}
	\item Time complexity \(O(N^2))\): This can be seen as for every data point in the dataset, every datapoint is again visited to find its neighbour. Optimizations like using indexes and parallelisation are used to optimize the running time.
	\item Space complexity \(O(N^2)\): For each data point all of its neighbour are stored. This overhead can be reduced by instead of storing the all the neighbours, whatever computation is needed must be done and stored and the cluster neighbours are thrown away. The only draw back is when doing a different computation, the neighbour will have to be found again. 
\end{itemize}

\section{Software}

This section introduces the software and the framework which were chosen to implement the software. The software basic requirements were to be on of the mainstream programming language, open source, cross-platform and which has long term community support. So the top options from which we had to choose from was Java, C/C++ and Python.

In \cite{hugunin1997Python}, vary nice comparison is done between Java, Python and C. When it comes to portability, Java programs can compile to portable executable bytecode which can be run on any computer as long as it supports Java virtual machine. ANSI C, which can achieve the same portability require re-compilation of the source files. This means Java programs can be distributed as binary files that can run on any platform where as C would require recompilation tools to recompile on the specific hardware. While Python code can enjoy this advantage and can run on any machine with a Python interpreter installed, the C-based Python extension modules, as well as the central Python interpreter itself, are only portable after (sometimes painful) recompilation of C source. With ease of portability and cross-platform we also know that Java program are robust and can never have segmentation fault, and most of the errors are catchable runtime exceptions. On the other hand in C uncatchable, destructive errors all too frequently at runtime \cite{hugunin1997Python}.

Garbage collection is supported by both Python and Java with the exception of C. Making it easier to code and build software. But since, performance is another aspect, C and Java tops Python because of the interpretated nature of the Python \cite{hugunin1997Python}. C is more performant than Java because of AOT (ahead of time compilation) in C than JIT (Just in time compilation) of Java. But writing and reading multi-threaded code is much easier in Java because of their nice set of portable API's and well documentation. Python GIL (global interpreter lock) limits thread performance vastly \cite{beazley2010understanding}.

Since, we have to search through a lot of records, a lot of operation like grouping, summing, counting were required to be done on the dataset. Storing the dataset on memory using array list, map any data structure would be hard as well as error prone with too much care needed for multi-threading. So, we decided to do the heavy dataset related operation from database. Which can later be seen to use indexes to fasten up the search. With a little performance penalty, we wouldnot have to worry about the memory limit anymore and disk space would be utilized by the database when not enough main memory is available. 

A number of database models are out there namely relational model (e.g. MySQL), document model (e.g. MongoDB), graph model (e.g. Neo4j) and multi-model (e.g. ArangoDB). The dataset provided to the software can be with any number of columns to analyse. Dataset will be in the form of \(k\) columns separated by comma with each column containing ``double'' values where \(k \in \mathbb{Z} \wedge k \geq 2\). So the storage of these datasets donot have pre-defined schema. Grouping operations is one of the most important operations required. Also, we want to be able to represent the operation with simple query language. ArangoDB stood out from all of the databases, relational databases has really readable query language but its strict schema negates are requirement. In document model MongoDB does support our dynamic model requirement but its query syntax is harder to read. Graph model is made for graph based analysis whereas our analysis donot requires graph. ArangoDB completes all of our requirement by supporting our dynamic model, having simple query language similar to MySQL named AQL and does support grouping operation providing graph operations as a bonus if ever required in the future.

With this, conclusion was made to use Java as the programming language and ArangoDB as the database for hard core data computing. Java has a large community support and ArangoDB fully supports Java client apps.

\section{Applications}

The reason for making a software like this verifies and gives the confidence about the dataset. If data do not fit any functional hypothesis in a space, much time could be saved by preventing the unneeded search. It also analysis the linear relation of the dataset. It assumes data is linear, does linear regression on various cluster and returns multitude of values describing the data (e.g. number of cluster formed, number of outliers, mean of Pearson's coefficient R etc.).

It can be used to confirm the linear models already presented by \cite{bircher2007complete}\cite{bellosa2000benefits}\cite{o2017survey}. The tool can be used to find linear relationship in the dataset. 

The software is not restricted to the use of only on dataset which consists of performance events and power consumption. It is a general-purpose software which will for work for any kind of dataset in which user wants to know the existence of functional relation. The refuting of the claim of functional relation on dataset is the objective of the software.

We also know that the dataset provided is usually experimentally measured values which are not accurate. Every measuring device has some margin of error. The software will be flexible in the sense that the equality comparison of values in the approaches will always be done keeping in the margin of error provided. It can also iterate certain of tolerance which will further be used to study the effect of tolerance on the dataset.
